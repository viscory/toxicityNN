{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viscory/toxicityNN/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "57536d12-77fb-43be-c8f1-c79e43f03a9b",
        "id": "gb09VoBOie36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "train_dataset = np.genfromtxt(PATH + \"train_labels.csv\", delimiter = ',', usecols = (1))\n",
        "train_dataset = train_dataset.tolist()\n",
        "train_dataset.pop(0)\n",
        "\n",
        "test_dataset = np.genfromtxt(PATH + \"test_labels.csv\", delimiter = ',', usecols = (1))\n",
        "test_dataset = test_dataset.tolist()\n",
        "test_dataset.pop(0)\n",
        "\n",
        "Y_train = [1 if i == 1.0 else 0 for i in train_dataset]\n",
        "Y_test = [1 if i == 1.0 else 0 for i in test_dataset]\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, num_classes = 2)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, num_classes = 2)\n",
        "\n",
        "train_graph = np.genfromtxt(PATH + \"train_graphs.csv\", delimiter = ',')\n",
        "train_graph_size = np.genfromtxt(PATH + \"train_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "train_nodes = np.genfromtxt(PATH + \"train_nodes.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "\n",
        "test_graph = np.genfromtxt(PATH + \"test_graphs.csv\", delimiter = ',')\n",
        "test_graph_size = np.genfromtxt(PATH + \"test_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "test_nodes = np.genfromtxt(PATH + \"test_nodes.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "\n",
        "# gap\n",
        "\n",
        "X_train = np.zeros([train_graph_size.size, 134, 132], dtype = int)\n",
        "X_test = np.zeros([test_graph_size.size, 134, 132], dtype = int)\n",
        "\n",
        "for i in range(train_graph_size.size):\n",
        "    pt1 = train_graph[132*i:132*(i+1)]\n",
        "    pt2 = np.reshape([char for char in train_nodes.tolist()[(132*i):(132*(i+1))]], [1,132])\n",
        "    pt3 = np.zeros([132,1], dtype = int)\n",
        "    pt3[0] = train_graph_size[i]\n",
        "    pt3 = np.reshape(pt3, [1,132])\n",
        "    pt1 /= np.max(pt1)\n",
        "    pt2 /= np.max(pt2)\n",
        "    # pt3 /= np.max(pt3)\n",
        "    pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "    X_train[i] = pt\n",
        "    # print('done')\n",
        "print('doneE')\n",
        "\n",
        "for i in range(test_graph_size.size):\n",
        "    pt1 = test_graph[132*i:132*(i+1)]\n",
        "    pt2 = np.reshape([char for char in test_nodes.tolist()[(132*i):(132*(i+1))]], [1,132])\n",
        "    pt3 = np.zeros([132,1], dtype = int)\n",
        "    pt3[0] = test_graph_size[i]\n",
        "    pt3 = np.reshape(pt3, [1,132])\n",
        "    pt1 /= np.max(pt1)\n",
        "    pt2 /= np.max(pt2)\n",
        "    # pt3 /= np.max(pt3)\n",
        "    pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "    X_test[i] = pt\n",
        "\n",
        "X_train = np.reshape(X_train, [len(X_train), 134, 132, 1])\n",
        "X_test = np.reshape(X_test, [len(X_test), 134, 132, 1])\n",
        "\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add( tf.keras.layers.Conv2D(16, 4, input_shape=(134, 132, 1), activation='relu') )\n",
        "model.add( tf.keras.layers.MaxPooling2D(2) )\n",
        "model.add( tf.keras.layers.Conv2D(32, 4, activation='relu') )\n",
        "model.add( tf.keras.layers.MaxPooling2D(2) )\n",
        "model.add( tf.keras.layers.Flatten() )\n",
        "model.add( tf.keras.layers.Dropout(0.4) )\n",
        "model.add( tf.keras.layers.Dense(100, activation='relu') )\n",
        "model.add( tf.keras.layers.Dense(2, activation='softmax') )\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(lr=0.001), metrics=['acc'])\n",
        "history_best = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), batch_size=200, epochs=10, class_weight = {0: 1, 1:20})\n",
        "predictions = model.predict(X_test)\n",
        "model.save(\"network.h5\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "doneE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 131, 129, 16)      272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 65, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 62, 61, 32)        8224      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 31, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 29760)             0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 29760)             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               2976100   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 2,984,798\n",
            "Trainable params: 2,984,798\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 5617 samples, validate on 2808 samples\n",
            "Epoch 1/10\n",
            "5617/5617 [==============================] - 2s 353us/sample - loss: 41071355952348.0547 - acc: 0.7073 - val_loss: 0.3602 - val_acc: 0.9448\n",
            "Epoch 2/10\n",
            "5617/5617 [==============================] - 2s 282us/sample - loss: 1.0448 - acc: 0.8239 - val_loss: 0.4588 - val_acc: 0.9021\n",
            "Epoch 3/10\n",
            "5617/5617 [==============================] - 2s 280us/sample - loss: 1.0163 - acc: 0.8065 - val_loss: 0.3581 - val_acc: 0.9345\n",
            "Epoch 4/10\n",
            "5617/5617 [==============================] - 2s 282us/sample - loss: 0.9805 - acc: 0.8337 - val_loss: 0.5833 - val_acc: 0.8768\n",
            "Epoch 5/10\n",
            "5617/5617 [==============================] - 2s 283us/sample - loss: 0.9566 - acc: 0.8257 - val_loss: 0.3303 - val_acc: 0.9487\n",
            "Epoch 6/10\n",
            "5617/5617 [==============================] - 2s 284us/sample - loss: 0.9521 - acc: 0.8375 - val_loss: 0.2656 - val_acc: 0.9523\n",
            "Epoch 7/10\n",
            "5617/5617 [==============================] - 2s 285us/sample - loss: 0.9057 - acc: 0.8519 - val_loss: 0.3604 - val_acc: 0.9441\n",
            "Epoch 8/10\n",
            "5617/5617 [==============================] - 2s 285us/sample - loss: 0.8825 - acc: 0.8624 - val_loss: 0.5579 - val_acc: 0.8650\n",
            "Epoch 9/10\n",
            "5617/5617 [==============================] - 2s 288us/sample - loss: 0.8843 - acc: 0.8357 - val_loss: 0.2134 - val_acc: 0.9605\n",
            "Epoch 10/10\n",
            "5617/5617 [==============================] - 2s 285us/sample - loss: 0.8553 - acc: 0.8659 - val_loss: 0.1937 - val_acc: 0.9623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhHTwScyoHDH",
        "colab_type": "code",
        "outputId": "9357142c-9c4c-4cdd-ad24-e02371233cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "count = 0\n",
        "with open('labels.txt', 'r') as filehandle:\n",
        "  array = filehandle.read().split('\\n')\n",
        "filehandle.close()\n",
        "# print(len(array))\n",
        "score_dataset = np.genfromtxt(PATH + \"score_labels.csv\", delimiter = ',', usecols = (1))\n",
        "score_dataset = score_dataset.tolist()\n",
        "score_dataset.pop(0)\n",
        "Y_score = [1 if i == 1.0 else 0 for i in score_dataset]\n",
        "array = [int(i) for i in array]\n",
        "count = 0\n",
        "for a, b in zip(array, Y_score):\n",
        "  if a == b:\n",
        "    count+=1\n",
        "print(count)\n",
        "273/(273+2535)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09722222222222222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFJ81DbkQ0qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras import models\n",
        "# from keras import layers\n",
        "# from keras.optimizers import RMSprop\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# import numpy as np\n",
        "\n",
        "# PATH = 'score/'\n",
        "\n",
        "# score_dataset = np.genfromtxt(PATH + \"score_labels.csv\", delimiter = ',', usecols = (1))\n",
        "# score_dataset = train_dataset.tolist()\n",
        "# score_dataset.pop(0)\n",
        "\n",
        "\n",
        "# Y_score = [1 if i == 1.0 else 0 for i in score_dataset]\n",
        "\n",
        "# score_graph = np.genfromtxt(PATH + \"score_graphs.csv\", delimiter = ',')\n",
        "# score_graph_size = np.genfromtxt(PATH + \"score_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "# score_nodes = np.genfromtxt(PATH + \"score_nodes.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "\n",
        "# # gap\n",
        "# X_score = np.zeros([test_graph_size.size, 134, 132], dtype = int)\n",
        "\n",
        "# for i in range(score_graph_size.size):\n",
        "#     pt1 = score_graph[132*i:132*(i+1)]\n",
        "#     pt2 = np.reshape([char for char in score_nodes.tolist()[(132*i):(132*(i+1))]], [1,132])\n",
        "#     pt3 = np.zeros([132,1], dtype = int)\n",
        "#     pt3[0] = score_graph_size[i]\n",
        "#     pt3 = np.reshape(pt3, [1,132])\n",
        "#     pt1 /= np.max(pt1)\n",
        "#     pt2 /= np.max(pt2)\n",
        "#     # pt3 /= np.max(pt3)\n",
        "#     pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "#     X_score[i] = pt\n",
        "#     # print('done')\n",
        "\n",
        "# X_score = np.reshape(X_score, [len(X_score), 134, 132, 1])\n",
        "\n",
        "# new_model = keras.models.load_model('network.h5')\n",
        "# new_model.predict(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok7HNhL61l84",
        "colab_type": "code",
        "outputId": "cb336fc1-7a1b-4cb0-8d8c-f2305ac9fca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "train_dataset = np.genfromtxt(PATH + \"train_labels.csv\", delimiter = ',', usecols = (1))\n",
        "train_dataset = train_dataset.tolist()\n",
        "train_dataset.pop(0)\n",
        "\n",
        "test_dataset = np.genfromtxt(PATH + \"test_labels.csv\", delimiter = ',', usecols = (1))\n",
        "test_dataset = test_dataset.tolist()\n",
        "test_dataset.pop(0)\n",
        "\n",
        "Y_train = [1 if i == 1.0 else 0 for i in train_dataset]\n",
        "Y_test = [1 if i == 1.0 else 0 for i in test_dataset]\n",
        "\n",
        "Y_train = keras.utils.to_categorical(Y_train, num_classes = 2)\n",
        "Y_test = keras.utils.to_categorical(Y_test, num_classes = 2)\n",
        "\n",
        "train_graph = np.genfromtxt(PATH + \"train_graphs.csv\", delimiter = ',')\n",
        "train_graph_size = np.genfromtxt(PATH + \"train_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "train_nodes = np.genfromtxt(PATH + \"train_nodes.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "\n",
        "test_graph = np.genfromtxt(PATH + \"test_graphs.csv\", delimiter = ',')\n",
        "test_graph_size = np.genfromtxt(PATH + \"test_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "test_nodes = np.genfromtxt(PATH + \"test_nodes.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "\n",
        "# gap\n",
        "\n",
        "X_train = np.zeros([train_graph_size.size, 134, 132], dtype = int)\n",
        "X_test = np.zeros([test_graph_size.size, 134, 132], dtype = int)\n",
        "\n",
        "for i in range(train_graph_size.size):\n",
        "    pt1 = train_graph[132*i:132*(i+1)]\n",
        "    pt2 = np.reshape([char for char in train_nodes.tolist()[(132*i):(132*(i+1))]], [1,132])\n",
        "    pt3 = np.zeros([132,1], dtype = int)\n",
        "    pt3[0] = train_graph_size[i]\n",
        "    pt3 = np.reshape(pt3, [1,132])\n",
        "    pt1 /= np.max(pt1)\n",
        "    pt2 /= np.max(pt2)\n",
        "    # pt3 /= np.max(pt3)\n",
        "    pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "    X_train[i] = pt\n",
        "    # print('done')\n",
        "print('doneE')\n",
        "\n",
        "for i in range(test_graph_size.size):\n",
        "    pt1 = test_graph[132*i:132*(i+1)]\n",
        "    pt2 = np.reshape([char for char in test_nodes.tolist()[(132*i):(132*(i+1))]], [1,132])\n",
        "    pt3 = np.zeros([132,1], dtype = int)\n",
        "    pt3[0] = test_graph_size[i]\n",
        "    pt3 = np.reshape(pt3, [1,132])\n",
        "    pt1 /= np.max(pt1)\n",
        "    pt2 /= np.max(pt2)\n",
        "    # pt3 /= np.max(pt3)\n",
        "    pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "    X_test[i] = pt\n",
        "\n",
        "X_train = np.reshape(X_train, [len(X_train), 134, 132, 1])\n",
        "X_test = np.reshape(X_test, [len(X_test), 134, 132, 1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "doneE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTXgWnEDxjB5",
        "colab_type": "code",
        "outputId": "2b9931e9-c222-46b5-da1b-ddf63c9e8c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# # # # # # np.reshape(train_nodes[132 * i: 132 * (i+1)], [132,1])\n",
        "# # # # # # train_nodes[132 * i: 132 * (i+1)]\n",
        "\n",
        "# # # # train_graph_size = np.genfromtxt(PATH + \"train_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "# # # # # train_graph_size = np.reshape(train_graph_size, )\n",
        "# # # # pt4 = np.zeros([132,1], dtype = int)\n",
        "# # # # pt4[0] = train_graph_size[0]\n",
        "# # # # pt4\n",
        "\n",
        "# # #     # pt1 = train_graph[132 * i : 132 * (i+1),]\n",
        "# # #     # pt2 = np.reshape(train_nodes[132 * i: 132 * (i+1)], [132,1])\n",
        "# # #     # pt3 = np.append(pt1, pt2, axis = 1)\n",
        "# # #     # pt4 = np.zeros([132,1], dtype = int)\n",
        "# # #     # pt4[0] = train_graph_size[i]\n",
        "# # #     # pt1 = np.append(pt3, pt4, axis = 1)\n",
        "# # #     # X_train[i] = pt1\n",
        "# # # # train_graph = np.genfromtxt(PATH + \"train_graphs.csv\", delimiter = ',')\n",
        "# # # # train_graph_size = np.genfromtxt(PATH + \"train_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "# # # # train_nodes = np.genfromtxt(PATH + \"train_nodes.csv\", delimiter = ',')\n",
        "# # # # i=1\n",
        "# # # pt1 = train_graph[132*i:132*(i+1)]\n",
        "# # # # train_nodes\n",
        "# # # pt2 = np.reshape(np.asarray([c[1] for c in train_nodes[(1+132*i):(1+132*(i+1))]]), [1,132])\n",
        "# # # pt4 = np.zeros([132,1], dtype = int)\n",
        "# # # pt4[0] = train_graph_size[i]\n",
        "# # # pt4 = np.reshape(pt4, [1,132])\n",
        "# # # # pt4\n",
        "# # # # pt4.shape\n",
        "# # # pt=np.append(np.append(pt4, pt2, axis=0), pt1, axis=0)\n",
        "# # # # pt2\n",
        "# # # # pt1.shape\n",
        "# # # pt.shape\n",
        "# # # \n",
        "# # if 1==1:\n",
        "# #     pt1 = train_graph[132*i:132*(i+1)]\n",
        "# #     pt2 = np.reshape(np.asarray([c[1] for c in train_nodes[(1+132*i):(1+132*(i+1))]]), [1,132])\n",
        "# #     pt3 = np.zeros([132,1], dtype = int)\n",
        "# #     pt3[0] = train_graph_size[i]\n",
        "# #     pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "# if i==1:\n",
        "#     pt1 = train_graph[132*i:132*(i+1)]\n",
        "#     pt1 /= np.max(pt1)\n",
        "#     pt2 = np.reshape([char for char in train_nodes.tolist()[(132*i):(132*(i+1))]], [1,132])\n",
        "#     pt3 = np.zeros([132,1], dtype = int)\n",
        "#     pt3[0] = train_graph_size[i]\n",
        "#     pt1 /= np.max{pt1}\n",
        "#     pt2 /= np.max(pt2)\n",
        "#     pt3 /= np.max(pt3)\n",
        "#     pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "# np.max(pt1)\n",
        "# # # X_train = np.reshape(X_train, [len(X_train), 1, 132, 134])\n",
        "# # # X_test = np.reshape(X_test, [len(X_test), 1, 132, 134])\n",
        "# X_train.shape\n",
        "# count1 = 0\n",
        "# count2 = 0\n",
        "# for x in Y_train:\n",
        "#   if x[1] == 0:\n",
        "#     count2 += 1\n",
        "#   else:\n",
        "#     count1 += 1\n",
        "# print (count1 + count2)\n",
        "# Y_train[0][1]\n",
        "# y_train = [int(y[1]) for y in Y_train]\n",
        "# y_train\n",
        "Y_train = [1 if i == 1.0 else 0 for i in train_dataset]\n",
        "print(len(Y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc-G0z2YHiOY",
        "colab_type": "code",
        "outputId": "63fe3a40-cb39-4808-b357-44a83dbf3dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add( layers.Conv2D(16, 4, input_shape=(134, 132, 1), activation='relu') )\n",
        "model.add( layers.MaxPooling2D(2) )\n",
        "model.add( layers.Conv2D(32, 4, activation='relu') )\n",
        "model.add( layers.MaxPooling2D(2) )\n",
        "model.add( layers.Flatten() )\n",
        "model.add( layers.Dropout(0.4) )\n",
        "model.add( layers.Dense(100, activation='relu') )\n",
        "model.add( layers.Dense(2, activation='softmax') )\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['acc'])\n",
        "history_best = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), batch_size=200, epochs=10, class_weight = {0: 1, 1:20})\n",
        "predictions = model.predict(X_test)\n",
        "model.save(\"networks.h5\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 131, 129, 16)      272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 65, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 62, 61, 32)        8224      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 31, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 29760)             0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 29760)             0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               2976100   \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 2,984,798\n",
            "Trainable params: 2,984,798\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 5617 samples, validate on 2808 samples\n",
            "Epoch 1/10\n",
            "5617/5617 [==============================] - 3s 453us/step - loss: 1.1877 - acc: 0.8567 - val_loss: 0.3951 - val_acc: 0.9402\n",
            "Epoch 2/10\n",
            "5617/5617 [==============================] - 2s 326us/step - loss: 1.1014 - acc: 0.7776 - val_loss: 0.3861 - val_acc: 0.9252\n",
            "Epoch 3/10\n",
            "5617/5617 [==============================] - 2s 333us/step - loss: 1.0413 - acc: 0.7634 - val_loss: 0.2059 - val_acc: 0.9580\n",
            "Epoch 4/10\n",
            "5617/5617 [==============================] - 2s 331us/step - loss: 1.0399 - acc: 0.7618 - val_loss: 0.3894 - val_acc: 0.9042\n",
            "Epoch 5/10\n",
            "5617/5617 [==============================] - 2s 331us/step - loss: 0.9854 - acc: 0.7816 - val_loss: 0.1747 - val_acc: 0.9605\n",
            "Epoch 6/10\n",
            "5617/5617 [==============================] - 2s 332us/step - loss: 0.9793 - acc: 0.7976 - val_loss: 0.2487 - val_acc: 0.9541\n",
            "Epoch 7/10\n",
            "5617/5617 [==============================] - 2s 330us/step - loss: 0.9465 - acc: 0.7933 - val_loss: 0.2823 - val_acc: 0.9373\n",
            "Epoch 8/10\n",
            "5617/5617 [==============================] - 2s 328us/step - loss: 0.9280 - acc: 0.7938 - val_loss: 0.1970 - val_acc: 0.9594\n",
            "Epoch 9/10\n",
            "5617/5617 [==============================] - 2s 325us/step - loss: 0.9127 - acc: 0.8031 - val_loss: 0.2101 - val_acc: 0.9619\n",
            "Epoch 10/10\n",
            "5617/5617 [==============================] - 2s 324us/step - loss: 0.8994 - acc: 0.8100 - val_loss: 0.4344 - val_acc: 0.9085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13Ybl4_LGrWN",
        "colab_type": "code",
        "outputId": "5b329a80-2f32-4f2f-f584-420ee019de03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dREgiPeOJPa",
        "colab_type": "code",
        "outputId": "c99ccaab-fed6-4fe7-ad30-2b423732695e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "prediction = model.predict(X_test)\n",
        "# print(prediction)\n",
        "# print(len(prediction[0]))\n",
        "prediction = [1.0 if i[1]>0.5 else 0.0 for i in prediction]\n",
        "actual = [1.0 if i[1]>0.5 else 0.0 for i in Y_test]\n",
        "tp = tn = fp = fn = 0 \n",
        "\n",
        "for x, y in zip(prediction, actual):\n",
        "  if x == y:\n",
        "    if x == 1.0:\n",
        "      tp += 1\n",
        "    else:\n",
        "      tn += 1\n",
        "  else:\n",
        "    if x == 1.0:\n",
        "      fp += 1\n",
        "    else:\n",
        "      fn += 1\n",
        "# type(tp/(tp+fp))\n",
        "try:\n",
        "  a = float((tp/(tp+fp)))\n",
        "except:\n",
        "  a = 0\n",
        "try:\n",
        "  b = float((tn/(tn+fn)))\n",
        "except:\n",
        "  b = 0\n",
        "print(0.5*(a+b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.613871962168016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzMzkX9aCbai",
        "colab_type": "code",
        "outputId": "2540997d-d955-4e9c-8f17-28f60e660906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "\n",
        "new_model = keras.models.load_model('networks.h5')\n",
        "new_model.predict(X_test)\n",
        "prediction = new_model.predict(X_train)\n",
        "# print(prediction)\n",
        "# print(len(prediction[0]))\n",
        "prediction = [1.0 if i[1]>0.5 else 0.0 for i in prediction]\n",
        "actual = [1.0 if i[1]>0.5 else 0.0 for i in Y_train]\n",
        "tp = tn = fp = fn = 0 \n",
        "\n",
        "for x, y in zip(prediction, actual):\n",
        "  if x == y:\n",
        "    if x == 1.0:\n",
        "      tp += 1\n",
        "    else:\n",
        "      tn += 1\n",
        "  else:\n",
        "    if x == 1.0:\n",
        "      fp += 1\n",
        "    else:\n",
        "      fn += 1\n",
        "# type(tp/(tp+fp))\n",
        "a = float((tp/(tp+fn)))\n",
        "b = float((tn/(tn+fp)))\n",
        "print(0.5*(a+b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e532839f67ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'networks.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW3m7CDISMFf",
        "colab_type": "code",
        "outputId": "b3e42352-a8e9-4ce9-ceae-c9ec60d6c1b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "train_dataset = np.genfromtxt(PATH + \"train_labels.csv\", delimiter = ',', usecols = (1))\n",
        "train_dataset = train_dataset.tolist()\n",
        "train_dataset.pop(0)\n",
        "\n",
        "test_dataset = np.genfromtxt(PATH + \"test_labels.csv\", delimiter = ',', usecols = (1))\n",
        "test_dataset = test_dataset.tolist()\n",
        "test_dataset.pop(0)\n",
        "\n",
        "Y_train = [1 if i == 1.0 else 0 for i in train_dataset]\n",
        "Y_test = [1 if i == 1.0 else 0 for i in test_dataset]\n",
        "\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, num_classes = 2)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, num_classes = 2)\n",
        "\n",
        "train_graph = np.genfromtxt(PATH + \"train_graphs.csv\", delimiter = ',')\n",
        "train_graph_size = np.genfromtxt(PATH + \"train_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "train_nodes = np.genfromtxt(PATH + \"train_nodes.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "\n",
        "test_graph = np.genfromtxt(PATH + \"test_graphs.csv\", delimiter = ',')\n",
        "test_graph_size = np.genfromtxt(PATH + \"test_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "test_nodes = np.genfromtxt(PATH + \"test_nodes.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "\n",
        "# gap\n",
        "\n",
        "X_train = np.zeros([train_graph_size.size, 134, 132], dtype = int)\n",
        "X_test = np.zeros([test_graph_size.size, 134, 132], dtype = int)\n",
        "\n",
        "for i in range(train_graph_size.size):\n",
        "    pt1 = train_graph[132*i:132*(i+1)]\n",
        "    pt2 = np.reshape([char for char in train_nodes.tolist()[(132*i):(132*(i+1))]], [1,132])\n",
        "    pt3 = np.zeros([132,1], dtype = int)\n",
        "    pt3[0] = train_graph_size[i]\n",
        "    pt3 = np.reshape(pt3, [1,132])\n",
        "    pt1 /= np.max(pt1)\n",
        "    pt2 /= np.max(pt2)\n",
        "    # pt3 /= np.max(pt3)\n",
        "    pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "    X_train[i] = pt\n",
        "    # print('done')\n",
        "print('doneE')\n",
        "\n",
        "for i in range(test_graph_size.size):\n",
        "    pt1 = test_graph[132*i:132*(i+1)]\n",
        "    pt2 = np.reshape([char for char in test_nodes.tolist()[(132*i):(132*(i+1))]], [1,132])\n",
        "    pt3 = np.zeros([132,1], dtype = int)\n",
        "    pt3[0] = test_graph_size[i]\n",
        "    pt3 = np.reshape(pt3, [1,132])\n",
        "    pt1 /= np.max(pt1)\n",
        "    pt2 /= np.max(pt2)\n",
        "    # pt3 /= np.max(pt3)\n",
        "    pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "    X_test[i] = pt\n",
        "\n",
        "X_train = np.reshape(X_train, [len(X_train), 134, 132, 1])\n",
        "X_test = np.reshape(X_test, [len(X_test), 134, 132, 1])\n",
        "\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add( tf.keras.layers.Conv2D(16, 4, input_shape=(134, 132, 1), activation='relu') )\n",
        "model.add( tf.keras.layers.MaxPooling2D(2) )\n",
        "model.add( tf.keras.layers.Conv2D(32, 4, activation='relu') )\n",
        "model.add( tf.keras.layers.MaxPooling2D(2) )\n",
        "model.add( tf.keras.layers.Flatten() )\n",
        "model.add( tf.keras.layers.Dropout(0.4) )\n",
        "model.add( tf.keras.layers.Dense(100, activation='relu') )\n",
        "model.add( tf.keras.layers.Dense(2, activation='softmax') )\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(lr=0.001), metrics=['acc'])\n",
        "history_best = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), batch_size=200, epochs=10, class_weight = {0: 1, 1:20})\n",
        "predictions = model.predict(X_test)\n",
        "model.save(\"network.h5\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "doneE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 131, 129, 16)      272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 65, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 62, 61, 32)        8224      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 31, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 29760)             0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 29760)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               2976100   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 2,984,798\n",
            "Trainable params: 2,984,798\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a45988633aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mhistory_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RMSprop' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77bFqWJZUy4S",
        "colab_type": "code",
        "outputId": "6548b549-4acc-4b29-d68c-68688834d347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "PATH = '/content/Tox21_AR/'\n",
        "\n",
        "score_dataset = np.genfromtxt(PATH + \"score_labels.csv\", delimiter = ',', usecols = (1))\n",
        "score_dataset = score_dataset.tolist()\n",
        "score_dataset.pop(0)\n",
        "\n",
        "Y_score = [1 if i == 1.0 else 0 for i in score_dataset]\n",
        "\n",
        "Y_score = tf.keras.utils.to_categorical(Y_score, num_classes = 2)\n",
        "\n",
        "\n",
        "score_graph = np.genfromtxt(PATH + \"score_graphs.csv\", delimiter = ',')\n",
        "score_graph_size = np.genfromtxt(PATH + \"score_graph_size.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "score_nodes = np.genfromtxt(PATH + \"score_nodes.csv\", delimiter = ',', usecols = (1))[1:]\n",
        "\n",
        "X_score = np.zeros([score_graph_size.size, 134, 132], dtype = int)\n",
        "X_score = np.zeros([score_graph_size.size, 134, 132], dtype = int)\n",
        "\n",
        "for i in range(score_graph_size.size):\n",
        "    pt1 = score_graph[132*i:132*(i+1)]\n",
        "    pt2 = np.reshape([char for char in score_nodes.tolist()[(132*i):(132*(i+1))]], [1,132])\n",
        "    pt3 = np.zeros([132,1], dtype = int)\n",
        "    pt3[0] = score_graph_size[i]\n",
        "    pt3 = np.reshape(pt3, [1,132])\n",
        "    pt1 /= np.max(pt1)\n",
        "    pt2 /= np.max(pt2)\n",
        "    # pt3 /= np.max(pt3)\n",
        "    pt = np.append(np.append(pt3, pt2, axis=0), pt1, axis=0)\n",
        "    X_score[i] = pt\n",
        "\n",
        "X_score = np.reshape(X_score, [len(X_score), 134, 132, 1])\n",
        "\n",
        "new_model = tf.keras.models.load_model('networks.h5')\n",
        "new_model.predict(X_score)\n",
        "prediction = new_model.predict(X_score)\n",
        "# print(prediction)\n",
        "# print(len(prediction[0]))\n",
        "prediction = [1 if i[1]>0.5 else 0 for i in prediction]\n",
        "with open('labels.txt', 'w+') as filehandle:\n",
        "    filehandle.writelines(\"%s\\n\" % label for label in predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1c194c521fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mscore_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"score_graphs.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mscore_graph_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"score_graph_size.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mscore_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"score_nodes.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   2087\u001b[0m             \u001b[0;31m# Raise an exception ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minvalid_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2089\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2090\u001b[0m             \u001b[0;31m# Issue a warning ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some errors were detected !\n    Line #2655 (got 152 columns instead of 132)\n    Line #5310 (got 62 columns instead of 132)\n    Line #7965 (got 152 columns instead of 132)\n    Line #10619 (got 194 columns instead of 132)\n    Line #13273 (got 152 columns instead of 132)\n    Line #15928 (got 63 columns instead of 132)\n    Line #18583 (got 152 columns instead of 132)\n    Line #21237 (got 194 columns instead of 132)\n    Line #23892 (got 20 columns instead of 132)\n    Line #26547 (got 194 columns instead of 132)\n    Line #29201 (got 152 columns instead of 132)\n    Line #31856 (got 62 columns instead of 132)\n    Line #34511 (got 152 columns instead of 132)\n    Line #37165 (got 194 columns instead of 132)\n    Line #39819 (got 152 columns instead of 132)\n    Line #42474 (got 63 columns instead of 132)\n    Line #45129 (got 152 columns instead of 132)\n    Line #47784 (got 62 columns instead of 132)\n    Line #50439 (got 152 columns instead of 132)\n    Line #53093 (got 194 columns instead of 132)\n    Line #55747 (got 152 columns instead of 132)\n    Line #58402 (got 62 columns instead of 132)\n    Line #61057 (got 152 columns instead of 132)\n    Line #63711 (got 194 columns instead of 132)\n    Line #66366 (got 21 columns instead of 132)\n    Line #69021 (got 194 columns instead of 132)\n    Line #71675 (got 152 columns instead of 132)\n    Line #73339 (got 37 columns instead of 132)"
          ]
        }
      ]
    }
  ]
}